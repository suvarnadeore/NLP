{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in c:\\users\\gutli\\anaconda3\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\users\\gutli\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied, skipping upgrade: Cython==0.29.21 in c:\\users\\gutli\\anaconda3\\lib\\site-packages (from gensim) (0.29.21)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in c:\\users\\gutli\\anaconda3\\lib\\site-packages (from gensim) (5.0.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in c:\\users\\gutli\\anaconda3\\lib\\site-packages (from gensim) (1.20.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Error while checking for conflicts. Please file an issue on pip's issue tracker: https://github.com/pypa/pip/issues/new\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gutli\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3021, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"C:\\Users\\gutli\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gutli\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3012, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"C:\\Users\\gutli\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gutli\\Anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 535, in _determine_conflicts\n",
      "    return check_install_conflicts(to_install)\n",
      "  File \"C:\\Users\\gutli\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\check.py\", line 108, in check_install_conflicts\n",
      "    package_set, _ = create_package_set_from_installed()\n",
      "  File \"C:\\Users\\gutli\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\check.py\", line 50, in create_package_set_from_installed\n",
      "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
      "  File \"C:\\Users\\gutli\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2736, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"C:\\Users\\gutli\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3023, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"C:\\Users\\gutli\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3032, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"C:\\Users\\gutli\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3014, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"C:\\Users\\gutli\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 1420, in get_metadata\n",
      "    value = self._get(path)\n",
      "  File \"C:\\Users\\gutli\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 1616, in _get\n",
      "    with open(path, 'rb') as stream:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'c:\\\\users\\\\gutli\\\\anaconda3\\\\lib\\\\site-packages\\\\google_pasta-0.2.0.dist-info\\\\METADATA'\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the data\n",
    "import re\n",
    "text=re.sub(r'\\[[0-9]*\\]',' ',paragraph)\n",
    "text=re.sub(r'\\s+',' ',text)\n",
    "text=text.lower()\n",
    "text=re.sub(r'\\d',' ',text)\n",
    "text=re.sub(r'\\s+',' ',text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the dataset\n",
    "sentences=nltk.sent_tokenize(text)\n",
    "\n",
    "sentences=[nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i]=[word for word in sentences[i] if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['three', 'visions', 'india', '.'],\n",
       " ['years',\n",
       "  'history',\n",
       "  ',',\n",
       "  'people',\n",
       "  'world',\n",
       "  'come',\n",
       "  'invaded',\n",
       "  'us',\n",
       "  ',',\n",
       "  'captured',\n",
       "  'lands',\n",
       "  ',',\n",
       "  'conquered',\n",
       "  'minds',\n",
       "  '.'],\n",
       " ['alexander',\n",
       "  'onwards',\n",
       "  ',',\n",
       "  'greeks',\n",
       "  ',',\n",
       "  'turks',\n",
       "  ',',\n",
       "  'moguls',\n",
       "  ',',\n",
       "  'portuguese',\n",
       "  ',',\n",
       "  'british',\n",
       "  ',',\n",
       "  'french',\n",
       "  ',',\n",
       "  'dutch',\n",
       "  ',',\n",
       "  'came',\n",
       "  'looted',\n",
       "  'us',\n",
       "  ',',\n",
       "  'took',\n",
       "  '.'],\n",
       " ['yet', 'done', 'nation', '.'],\n",
       " ['conquered', 'anyone', '.'],\n",
       " ['grabbed',\n",
       "  'land',\n",
       "  ',',\n",
       "  'culture',\n",
       "  ',',\n",
       "  'history',\n",
       "  'tried',\n",
       "  'enforce',\n",
       "  'way',\n",
       "  'life',\n",
       "  '.'],\n",
       " ['?'],\n",
       " ['respect', 'freedom', 'others.that', 'first', 'vision', 'freedom', '.'],\n",
       " ['believe',\n",
       "  'india',\n",
       "  'got',\n",
       "  'first',\n",
       "  'vision',\n",
       "  ',',\n",
       "  'started',\n",
       "  'war',\n",
       "  'independence',\n",
       "  '.'],\n",
       " ['freedom', 'must', 'protect', 'nurture', 'build', '.'],\n",
       " ['free', ',', 'one', 'respect', 'us', '.'],\n",
       " ['second', 'vision', 'india', '’', 'development', '.'],\n",
       " ['fifty', 'years', 'developing', 'nation', '.'],\n",
       " ['time', 'see', 'developed', 'nation', '.'],\n",
       " ['among', 'top', 'nations', 'world', 'terms', 'gdp', '.'],\n",
       " ['percent', 'growth', 'rate', 'areas', '.'],\n",
       " ['poverty', 'levels', 'falling', '.'],\n",
       " ['achievements', 'globally', 'recognised', 'today', '.'],\n",
       " ['yet',\n",
       "  'lack',\n",
       "  'self-confidence',\n",
       "  'see',\n",
       "  'developed',\n",
       "  'nation',\n",
       "  ',',\n",
       "  'self-reliant',\n",
       "  'self-assured',\n",
       "  '.'],\n",
       " ['’', 'incorrect', '?'],\n",
       " ['third', 'vision', '.'],\n",
       " ['india', 'must', 'stand', 'world', '.'],\n",
       " ['believe',\n",
       "  'unless',\n",
       "  'india',\n",
       "  'stands',\n",
       "  'world',\n",
       "  ',',\n",
       "  'one',\n",
       "  'respect',\n",
       "  'us',\n",
       "  '.'],\n",
       " ['strength', 'respects', 'strength', '.'],\n",
       " ['must', 'strong', 'military', 'power', 'also', 'economic', 'power', '.'],\n",
       " ['must', 'go', 'hand-in-hand', '.'],\n",
       " ['good', 'fortune', 'worked', 'three', 'great', 'minds', '.'],\n",
       " ['dr.', 'vikram', 'sarabhai', 'dept', '.'],\n",
       " ['space',\n",
       "  ',',\n",
       "  'professor',\n",
       "  'satish',\n",
       "  'dhawan',\n",
       "  ',',\n",
       "  'succeeded',\n",
       "  'dr.',\n",
       "  'brahm',\n",
       "  'prakash',\n",
       "  ',',\n",
       "  'father',\n",
       "  'nuclear',\n",
       "  'material',\n",
       "  '.'],\n",
       " ['lucky',\n",
       "  'worked',\n",
       "  'three',\n",
       "  'closely',\n",
       "  'consider',\n",
       "  'great',\n",
       "  'opportunity',\n",
       "  'life',\n",
       "  '.'],\n",
       " ['see', 'four', 'milestones', 'career']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the word2vec model\n",
    "model=Word2Vec(sentences,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1534d948>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding word vectors\n",
    "vector=model.wv['war']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00219905, -0.00970885,  0.00929075,  0.00203636, -0.00116388,\n",
       "       -0.00551955, -0.0085126 , -0.00989383,  0.00894091, -0.00250522,\n",
       "        0.00459427, -0.00452481,  0.00995189,  0.00366171,  0.00103129,\n",
       "       -0.00403834,  0.00122027, -0.00265451,  0.00735284,  0.00447542,\n",
       "        0.00099955,  0.0034782 ,  0.00372712, -0.00680036,  0.00893242,\n",
       "        0.00173499, -0.00579935,  0.00866838, -0.00129286,  0.00818304,\n",
       "       -0.0014927 ,  0.00698649,  0.00273452, -0.00436226, -0.00374683,\n",
       "        0.00919046,  0.00159645, -0.00599784,  0.00034776, -0.00195135,\n",
       "        0.00159242, -0.00771525,  0.00738298,  0.00131083,  0.00787099,\n",
       "        0.00445568, -0.00439675,  0.00376054, -0.0006357 , -0.00984484,\n",
       "        0.00825004,  0.00964326,  0.00965426, -0.00379659, -0.00844202,\n",
       "        0.00483581, -0.00765107,  0.00853567,  0.00275977,  0.00560496,\n",
       "        0.00611362,  0.00046455, -0.00209463,  0.000778  ,  0.00983559,\n",
       "       -0.00711718, -0.00155744, -0.00235984,  0.00487084,  0.00645515,\n",
       "       -0.0041403 ,  0.00361816, -0.00447258,  0.00326611,  0.0081738 ,\n",
       "        0.00361967, -0.0045711 , -0.00301938,  0.00787179,  0.00959686,\n",
       "        0.00580789, -0.00326881, -0.00183876, -0.00624998, -0.00429521,\n",
       "        0.00336554, -0.00648911, -0.00661903,  0.00811393,  0.00950739,\n",
       "        0.00814451,  0.00150699, -0.00880125, -0.00759764,  0.0015789 ,\n",
       "       -0.00952675, -0.00741688,  0.00203283, -0.00292885, -0.00916266],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most similar words\n",
    "similar=model.wv.most_similar('vikram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('visions', 0.18146508932113647),\n",
       " ('growth', 0.16634944081306458),\n",
       " ('one', 0.1643451601266861),\n",
       " ('took', 0.1643296778202057),\n",
       " (',', 0.15887506306171417),\n",
       " ('fifty', 0.14726699888706207),\n",
       " ('developing', 0.147140234708786),\n",
       " ('worked', 0.13810545206069946),\n",
       " ('development', 0.1376984417438507),\n",
       " ('time', 0.13293509185314178)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
